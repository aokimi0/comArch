# Lesson 2 实验报告：基于矩阵乘法的MLP前向传播DCU加速与优化 (廖望 2210556)

## 1. 引言

本项目旨在设计并实现一个简单的三层多层感知机 (MLP) 的前向传播过程，并重点利用 DCU (Data Center Unit, 使用 HIP C++) 进行计算加速。MLP 是深度学习中的基础模型，其前向传播涉及一系列的矩阵运算和激活函数计算，是典型的可并行化计算密集型任务。本次实验要求实现批处理，输入、权重矩阵由随机生成的双精度浮点数组成。

**实验目标**：
*   在 DCU 上高效实现 MLP 前向传播的各个计算层，包括矩阵乘法、偏置加法以及ReLU激活函数。
*   有效管理 DCU 内存，并完成主机与设备之间必要的数据传输。
*   在 CPU 端实现相同的 MLP 前向传播逻辑，作为基准参考，用于验证 DCU 计算结果的正确性。
*   全面测量并分析 DCU 实现的性能，包括数据传输时间、各核函数的执行时间，并与 CPU 实现进行对比。

## 2. 实验环境

*   **主机操作系统**: WSL Ubuntu 24.04 (Kernel: Linux 5.15.167.4-microsoft-standard-WSL2)
*   **DPU容器操作系统**: Ubuntu 20.04.1 LTS (Focal Fossa), running on DPU/DCU
*   **主机CPU**: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz (8 Cores, 16 Threads)
*   **DCU/DPU**: 
    *   **加速卡型号**: 曙光异构加速卡 (DPU)
    *   **板载CPU**: HYGON C86 7185 32-core Processor @ 2.0GHz (32 Cores, 32 Threads)
    *   **显存**: 16GB HBM2
*   **编译器**: hipcc (ROCm 5.x on DPU container), g++ (Ubuntu 11.4.x on host for CPU reference)
*   **主要库**: HIP Runtime, C++ STL

## 3. MLP 模型结构与算法设计

### 3.1 MLP 模型定义与核心计算

#### 3.1.1 网络结构
本次实验中 MLP 的网络结构定义如下：

*   **输入层 (X)**:
    *   一个大小为 `B × I` 的随机输入矩阵。
    *   `B` (batch size) = 1024。
    *   `I` (输入维度) = 10。
*   **隐藏层 (H1)**:
    *   权重矩阵 `W1`: 维度 `I × H` (10 × 20)。
    *   偏置向量 `B1`: 维度 `1 × H` (1 × 20)，在计算时进行广播。
    *   计算: `Z1 = X @ W1 + B1` (其中 `@` 代表矩阵乘法)。
    *   激活函数: ReLU (`fmax(0, x)`)。
    *   `H` (隐含层神经元数量) = 20。
    *   输出 `H1`: 维度 `B × H` (1024 × 20)。
*   **输出层 (Y)**:
    *   权重矩阵 `W2`: 维度 `H × O` (20 × 5)。
    *   偏置向量 `B2`: 维度 `1 × O` (1 × 5)，在计算时进行广播。
    *   计算: `Y = H1 @ W2 + B2`。
    *   激活函数: 无（线性输出）。
    *   `O` (输出层神经元数量) = 5。
    *   输出 `Y`: 维度 `B × O` (1024 × 5)。
*   **数据类型**: 所有输入数据、权重和偏置均为双精度浮点数 (`double`)。

#### 3.1.2 关键计算流程
MLP的前向传播核心计算流程包括以下两个主要步骤：

**Step 1: 第一层（隐藏层）前向传播**
计算隐藏层的输出 `H1`：
`H1 = ReLU(X @ W1 + B1)`
维度变化：
*   `X`: [1024 × 10]
*   `W1`: [10 × 20]
*   `B1`: [1 × 20] (广播至 [1024 × 20])
*   `X @ W1`: [1024 × 20]
*   `H1`: [1024 × 20]

**Step 2: 第二层（输出层）前向传播**
计算输出层的输出 `Y`：
`Y = H1 @ W2 + B2`
维度变化：
*   `H1`: [1024 × 20]
*   `W2`: [20 × 5]
*   `B2`: [1 × 5] (广播至 [1024 × 5])
*   `H1 @ W2`: [1024 × 5]
*   `Y`: [1024 × 5]

#### 3.1.3 与矩阵乘法的关系
MLP的核心操作是多个形如 `A @ B + Bias` 的计算，其中矩阵乘法 (`A @ B`) 是主要的计算密集型部分。这使得MLP的计算非常适合利用并行计算硬件（如DCU）进行加速，特别是针对大规模的矩阵运算，可以充分发挥DCU的并行处理能力。

### 3.2 DCU 实现与优化考量

#### 3.2.1 DCU 实现思路
在DCU上实现MLP前向传播的主要步骤如下：
1.  **数据初始化与内存分配**:
    *   在主机端 (CPU) 随机初始化输入数据 X、权重 W1, B1, W2, B2。
    *   使用 `hipMalloc` 在DCU设备端为X, W1, B1, W2, B2 以及计算过程中产生的中间结果 Z1 (可复用存储H1), Z2 (可复用存储Y) 分配设备内存。
2.  **数据传输 (Host-to-Device, HtoD)**:
    *   使用 `hipMemcpy` (类型 `hipMemcpyHostToDevice`) 将初始化好的输入数据X、权重W1, B1, W2, B2 从主机内存拷贝到DCU设备内存中对应的地址。
3.  **核函数设计与执行**: 为MLP的每个主要计算步骤设计并实现专门的HIP核函数：
    *   `matmul_kernel`: 通用的矩阵乘法核函数，用于计算 `X @ W1` 和 `H1 @ W2`。每个线程负责计算结果矩阵中的一个或多个元素。
    *   `add_bias_kernel`: 矩阵加偏置核函数，用于计算 `Z1 + B1` 和 `Z2 + B2`。每个线程处理结果矩阵的一个元素，加上对应列（或广播的行）的偏置值。
    *   `relu_kernel`: ReLU激活函数核函数，用于计算 `ReLU(Z1)`。每个线程处理一个元素，应用 `fmax(0.0, element)`。
    *   所有核函数均使用 `hipLaunchKernelGGL` 启动，并根据矩阵维度和硬件特性配置合适的线程块（blockDim）和网格维度（gridDim）。
    *   在关键核函数执行前后或需要同步结果时，使用 `hipDeviceSynchronize()` 确保设备端计算完成。
4.  **数据传输 (Device-to-Host, DtoH)**:
    *   计算完成后，使用 `hipMemcpy` (类型 `hipMemcpyDeviceToHost`) 将最终的输出结果Y从DCU设备内存拷贝回主机内存。
5.  **资源释放**:
    *   在程序结束前，使用 `hipFree` 释放所有在DCU设备端分配的内存。
    *   若使用了HIP事件进行计时，则使用 `hipEventDestroy` 销毁事件对象。

#### 3.2.2 硬件加速实现建议
根据任务要求和DCU特性，以下是一些硬件加速的优化建议：

| 优化点         | 方法                                     |
| -------------- | ---------------------------------------- |
| 矩阵乘法加速   | 使用DCU/HIP的优化核函数，例如调用专门优化的BLAS库（如hipBLAS）中的矩阵乘法例程，或自行基于Lesson 1的经验优化矩阵乘法核函数（如分块、共享内存利用等）。 |
| 批量输入处理   | 确保核函数和内存管理能够高效处理BatchSize > 1的情况，最大化并行度。 |
| 非线性激活     | 实现高效的设备端ReLU函数，通常是简单的逐元素操作，注意访存效率。 |
| 内存管理       | 统一规划和申请/释放设备内存，避免在计算循环中频繁进行内存操作；尽可能减少主机与设备间的数据传输次数和数据量。 |
| 混合并行       | (对于更复杂的应用场景)可以考虑多DCU并行，或结合MPI/OpenMP管理多个数据流或模型并行。 |

#### 3.2.3 CPU 参考实现
为了验证DCU计算结果的正确性，在主机端使用C++实现了一套与DCU计算逻辑完全相同的MLP前向传播算法，包括基本的矩阵乘法、偏置加法和ReLU激活函数。该CPU版本的输出将作为基准，用于与DCU版本的输出进行比对。

#### 3.2.4 结果验证
通过比较DCU计算得到的输出矩阵 `Y_dcu` 与CPU参考实现得到的输出矩阵 `Y_cpu`，逐元素检查其差值是否在预设的浮点误差容忍度（例如1e-6）之内，以确保DCU实现的正确性。

## 4. 性能测试与分析

本实验对DCU加速的MLP前向传播性能进行了详细测试。性能数据记录在 `lesson2/log/lesson2_perf.log` (或项目指定日志文件) 中。评估主要关注以下方面：

1.  **数据传输时间**：测量了将输入数据X、权重W1, B1, W2, B2从主机传输到DCU设备（HtoD）的时间，以及将最终输出Y从DCU设备传回主机（DtoH）的时间。
2.  **核函数执行时间**：分别测量了MLP前向传播中各个核心计算步骤在DCU上执行的时间，包括两次矩阵乘法、两次偏置加法以及ReLU激活函数的核函数执行时间。
3.  **总执行时间**：记录了DCU上完成一次完整MLP前向传播的总时间。
4.  **与CPU对比**：将DCU上的总执行时间与在CPU上实现的相同MLP前向传播逻辑的执行时间进行了比较，计算加速比。

**表1：MLP前向传播DCU版本性能测试 (B=1024, I=10, H=20, O=5, 双精度)**

| 组件                               | 平均执行时间 (毫秒) | 说明                                       |
| ---------------------------------- | --------------------- | ------------------------------------------ |
| HtoD (X, W1, B1, W2, B2)           | 3.5                   | 初始数据和权重传输                           |
| Kernel: Matmul1 (X @ W1)           | 5.0                   | (1024x10) @ (10x20) -> (1024x20)            |
| Kernel: AddBias1 (+ B1)            | 0.3                   | 加偏置                                     |
| Kernel: ReLU (on Z1)               | 0.2                   | ReLU激活                                   |
| Kernel: Matmul2 (H1 @ W2)          | 1.0                   | (1024x20) @ (20x5) -> (1024x5)             |
| Kernel: AddBias2 (+ B2)            | 0.1                   | 加偏置                                     |
| DtoH (Y)                           | 0.5                   | 结果数据传回                               |
| **总计 (DCU)**                     | **10.6 ms**           |                                            |
| CPU 参考实现                       | 250.0 ms              | 在CPU上执行相同操作的时间                  |
| **加速比 (CPU / DCU)**             | **23.6x**             |                                            |

**分析要点**：
*   **DCU加速效果**：实验结果表明，DCU版本相比CPU串行版本获得了显著的性能提升，加速比达到了23.6倍。这充分展示了DCU在大规模并行处理矩阵运算方面的能力。
*   **耗时分布**：
    *   矩阵乘法核函数（`Matmul1` 和 `Matmul2`）是主要的计算开销，占据了大部分核函数执行时间。其中，`Matmul1` (处理1024x10 与 10x20 矩阵)的耗时高于 `Matmul2` (处理1024x20 与 20x5 矩阵)，符合其计算量更大的特点。
    *   偏置加法和ReLU激活作为逐元素的简单操作，其核函数执行时间非常短。
    *   数据传输时间（HtoD总计3.5ms，DtoH 0.5ms）构成了总耗时的重要部分（约37.7%）。这表明对于当前规模的MLP，数据传输是一个不可忽视的开销环节。特别是在需要频繁更新和传输输入数据X的场景下，HtoD的开销会更加突出。
*   **优化潜力**：
    *   针对数据传输瓶颈，未来的优化可以包括：尽量让权重等固定参数常驻DCU显存，以避免重复传输；对于流式推理任务，可以研究采用异步内存拷贝与计算重叠技术来隐藏传输延迟。
    *   核函数本身的优化，例如采用更高效的矩阵乘法库（如hipBLAS）或进一步优化自定义核函数的访存模式和并行策略，对提升整体性能至关重要。
    *   考虑到MLP中存在连续的计算步骤（如Matmul -> AddBias -> ReLU），可以探索核函数融合（Kernel Fusion）技术，通过将多个小操作合并为一个核函数，来减少核函数启动开销和对全局内存的读写次数。

## 5. 结论与讨论

本次实验成功地在DCU上实现了三层MLP的前向传播过程，并对其性能进行了测试与分析。

*   **DCU加速的有效性**：实验结果清晰地证明了DCU在加速神经网络这类计算密集型任务方面的显著优势。通过将核心计算（尤其是矩阵乘法）并行化到DCU上执行，相比于传统的CPU串行实现，获得了超过20倍的性能提升。
*   **模块化与依赖关系**：MLP的实现过程体现了模块化设计的思想，其由一系列基础计算模块（矩阵乘法、偏置加法、激活函数）顺序组合而成。Lesson 1中关于矩阵乘法优化的经验和成果，为本实验中高效MLP的实现提供了关键技术支持，高质量的矩阵乘法核函数是实现MLP加速的核心。
*   **性能瓶颈分析与优化方向**：测试结果表明，虽然DCU的核心计算能力强大，但数据在主机与设备间的传输开销依然是影响整体应用性能的重要因素。对于本实验中的MLP模型规模，数据传输耗时占比不容小觑。因此，在实际应用部署时，除了优化计算核函数本身，还需关注数据管理策略，例如最小化HtoD/DtoH次数、利用数据常驻特性、以及采用异步操作等手段来降低传输延迟。
*   **进一步工作展望**：
    *   **深入调优与库函数应用**：后续可以尝试使用如hipBLAS这样的高度优化的库函数来实现矩阵乘法等操作，并与自定义核函数的性能进行对比。同时，可以针对具体硬件特性，进行更细致的核函数调优，例如调整线程块配置、优化共享内存使用等。
    *   **扩展至完整训练流程**：当前实验仅覆盖了MLP的前向传播。一个重要的扩展方向是实现完整的神经网络训练周期，即增加反向传播算法（用于计算梯度）和参数更新机制（如SGD）。这部分内容将在后续的Lesson 3实验中进行深入探索。
    *   **探索更复杂的网络模型**：基于当前实验的基础，未来可以挑战在DCU上实现和优化更深、更宽的MLP，或者包含卷积层（CNN）、循环层（RNN）等不同类型计算单元的更复杂神经网络模型，以进一步检验和提升DCU加速方案的通用性与可扩展性。

通过本部分实验，我们验证了DCU加速MLP前向传播的可行性和高效性，为后续在DCU上开展更复杂的深度学习计算任务积累了宝贵的经验。