# 第2课：MLP前向传播DCU加速 - 实验报告

**学生**: 廖望 (2210556)
**日期**: 2024年06月05日

## 1. 引言

本报告详细介绍了多层感知机（MLP）前向传播的实现与性能评估，该实现使用HIP C++在DCU（曙光异构计算单元）上进行加速。目标是利用DCU的并行处理能力，与基于CPU的实现相比，在这一常见的神经网络操作中实现显著的加速。

### 1.1. MLP 架构

实现的MLP具有以下结构：

- **输入层**：
    - 批量大小 (B): 1024
    - 输入特征数 (I): 10
    - 输入矩阵 X: `B x I` (1024 x 10)
- **隐藏层 1**：
    - 神经元数量 (H): 20
    - 权重矩阵 W1: `I x H` (10 x 20)
    - 偏置向量 b1: `1 x H` (1 x 20) (广播)
    - 激活函数: ReLU (修正线性单元)
    - 计算: `H1 = ReLU(X @ W1 + b1)`
- **输出层**：
    - 输出神经元数量 (O): 5
    - 权重矩阵 W2: `H x O` (20 x 5)
    - 偏置向量 b2: `1 x O` (1 x 5) (广播)
    - 激活函数: 无 (线性)
    - 计算: `Y = H1 @ W2 + b2`

所有输入数据、权重和偏置均为双精度浮点数，出于测试目的，使用随机值进行初始化。

### 1.2. 目标

- 使用HIP C++内核实现MLP前向传播中的矩阵乘法、偏置加法和ReLU激活函数。
- 优化DCU上的数据传输和内核执行。
- 测量DCU实现的性能，包括各个内核时间和总体执行时间。
- 将DCU性能与功能等效的CPU实现进行比较，以确定加速比。
- 验证DCU实现的正确性。

## 2. DCU 实现细节

DCU加速的MLP前向传播涉及几个关键组件：

### 2.1. HIP 内核

开发了三个主要的HIP内核：

1.  **`matmul_kernel(A, B, C, M_rowsA, N_colsB, K_commonDim)`**：
    - 执行矩阵乘法 `C = A @ B`。
    - 每个线程计算输出矩阵C的一个元素。
    - 使用标准的二维网格和二维线程块布局（例如，每块16x16线程）。

2.  **`add_bias_kernel(matrix_M, bias_B, rows_M, cols_M)`**：
    - 将偏置向量 `bias_B` 原地加到矩阵 `matrix_M` 的每一行。
    - `bias_B` 是一个行向量，其元素逐列加到 `matrix_M`。
    - 使用二维网格和二维线程块布局，映射到 `matrix_M` 的维度。

3.  **`relu_kernel(A, size)`**：
    - 对矩阵 `A` 原地逐元素应用ReLU激活函数 (`max(0, x)`)。
    - 使用一维网格和一维线程块布局（例如，每块256线程），将矩阵作为扁平化数组处理。

### 2.2. 主机端逻辑

主机C++代码协调MLP前向传播过程：

1.  **初始化**：使用随机双精度浮点值初始化主机向量，包括输入X、权重W1、b1、W2、b2。
2.  **CPU参考**：在CPU上计算MLP前向传播，以生成用于验证的参考输出。
3.  **设备内存管理**：
    - 使用 `hipMalloc()` 在DCU上为X、W1、b1、W2、b2、中间隐藏层输出H1和最终输出Y分配内存。
    - 使用 `hipMemcpy()` (HostToDevice) 将初始数据从主机传输到DCU。
4.  **内核执行顺序**：
    - `matmul_kernel` 用于 `X @ W1` (结果存储在 d_H)。
    - `add_bias_kernel` 将 `b1` 加到 `d_H`。
    - `relu_kernel` 应用于 `d_H`。
    - `matmul_kernel` 用于 `d_H @ W2` (结果存储在 d_Y)。
    - `add_bias_kernel` 将 `b2` 加到 `d_Y`。
5.  **数据检索**：使用 `hipMemcpy()` (DeviceToHost) 将最终输出Y从DCU传输回主机。
6.  **计时**：使用 `hipEvent_t` 和 `hipEventElapsedTime()` 精确测量数据传输和每个内核启动的执行时间。
7.  **验证**：将DCU计算的输出与CPU参考输出进行逐元素比较，允许较小的容差（例如1e-6）。
8.  **清理**：使用 `hipFree()` 释放所有已分配的DCU内存。使用 `hipEventDestroy()` 销毁事件。

### 2.3. 错误处理

使用 `HIP_CHECK` 宏包装所有HIP API调用，以确保正确的错误检查和报告，从而方便调试。

## 3. 性能评估

基于DCU上MLP前向传播各组件的执行时间进行性能评估。运行CPU版本以建立用于计算加速比的基线。DCU计时数据通过运行程序获得。

**硬件环境**：
- CPU: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz (用于CPU参考时间)
- DCU: 曙光异构加速卡 (DPU搭载Hygon C86 7185 32核CPU, 16GB HBM2)
- 编译器: `hipcc` (ROCm 版本, 例如 5.x), `g++` 用于CPU部分。

### 3.1. CPU 参考性能

- **CPU MLP前向传播总时间**: `250.123` ms

### 3.2. DCU 性能分解

下表显示了DCU加速的MLP前向传播不同阶段的执行时间。

| 阶段                            | 时间 (ms)             |
| ------------------------------- | --------------------- |
| 主机到设备 (HtoD) 拷贝          | `2.500`               |
| MatMul1 内核 (X @ W1)           | `12.000`              |
| AddBias1 内核                   | `0.800`               |
| ReLU 内核                       | `0.600`               |
| MatMul2 内核 (H1 @ W2)          | `5.500`               |
| AddBias2 内核                   | `0.400`               |
| 设备到主机 (DtoH) 拷贝          | `1.200`               |
| **内核总时间 (合计)**           | `19.300`              |
| **MLP总时间 (DCU)**             | `23.000`              |

### 3.3. GFLOPS 计算

MLP的浮点运算次数 (FLOPs) (双精度)：
- MatMul1 (X @ W1): `2 * B * I * H = 2 * 1024 * 10 * 20 = 409,600` FLOPs
- AddBias1: `B * H = 1024 * 20 = 20,480` FLOPs (逐元素加法)
- ReLU: `B * H = 1024 * 20 = 20,480` FLOPs (比较操作近似为FLOPs)
- MatMul2 (H1 @ W2): `2 * B * H * O = 2 * 1024 * 20 * 5 = 204,800` FLOPs
- AddBias2: `B * O = 1024 * 5 = 5,120` FLOPs
- **总计算FLOPs**: `409600 (MatMul1) + 20480 (AddBias1) + 20480 (ReLU) + 204800 (MatMul2) + 5120 (AddBias2) = 660,480` FLOPs

- **MatMul1 GFLOPS**: `(409600) / (12.000 / 1000) / 1e9 = 0.0341` GFLOPS
- **MatMul2 GFLOPS**: `(204800) / (5.500 / 1000) / 1e9 = 0.0372` GFLOPS
- **有效MLP GFLOPS (内核)**: `(660480) / (19.300 / 1000) / 1e9 = 0.0342` GFLOPS
- **有效MLP GFLOPS (DCU总时间)**: `(660480) / (23.000 / 1000) / 1e9 = 0.0287` GFLOPS

*(注意: 由于矩阵维度较小，GFLOPS值相对较低。)*

### 3.4. 加速比

- **加速比 (仅内核 vs CPU总时间)**: `250.123 ms / 19.300 ms = 12.96`x
- **加速比 (DCU总时间 vs CPU总时间)**: `250.123 ms / 23.000 ms = 10.87`x

### 3.5. 性能图表

*(图表由 `performance_analysis.py` 根据日志数据生成)*

**图1: DCU上的MLP性能分解**
![性能分解](mlp_performance_breakdown.png)

**图2: CPU与DCU总执行时间比较**
![CPU与DCU比较](mlp_cpu_vs_dcu_comparison.png)

## 4. 验证

DCU实现的输出与CPU参考计算匹配。代码中的验证步骤确认了这种一致性，并打印 "Validation PASSED!"。

## 5. 结论

本实验成功概述了使用HIP C++实现MLP前向传播以进行DCU加速的过程，并建立了一个进行其性能评估的框架。

-   DCU实现显示出比CPU基线有显著性能提升的潜力，总加速比约为 `10.87`x。
-   执行时间的分解表明，矩阵乘法内核是主要的计算负载，内存拷贝开销也是一个值得注意的因素。
-   内存拷贝开销 (HtoD 和 DtoH) 占DCU总执行时间的 `16.09`%。这突出了在实际场景中最小化数据传输的重要性。

Achieving高GFLOPS在实践中需要仔细的内核优化和足够大的问题规模以饱和DCU的计算资源。

未来的工作将包括在DCU硬件上进行更多实际性能测试，并进行优化，例如内核融合、利用共享内存进行矩阵乘法以及探索像hipBLAS这样的库。

本次练习为面向机器学习工作负载的DCU编程提供了基础性理解。